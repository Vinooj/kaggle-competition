{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vinooj/kaggle-competition/blob/main/housing_price_prediction/Ames_Housing_Price_Prediction_Kick_starter_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Research findings on how to tackle this problem: https://docs.google.com/document/d/1cQ9JuvhWk9c-moFPzfNjFl9qOZ5sJeuw74pBhjazHVA"
      ],
      "metadata": {
        "id": "tVhZl4823mdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn==1.4.2\n",
        "!pip install import-ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGWVfV1dFTWF",
        "outputId": "a4884902-a77f-457a-9923-e53bec2b28c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==1.4.2\n",
            "  Downloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (3.6.0)\n",
            "Downloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "Successfully installed scikit-learn-1.4.2\n",
            "Collecting import-ipynb\n",
            "  Downloading import_ipynb-0.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.11/dist-packages (from import-ipynb) (7.34.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from import-ipynb) (5.10.4)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (75.2.0)\n",
            "Collecting jedi>=0.16 (from IPython->import-ipynb)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (4.9.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->import-ipynb) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->import-ipynb) (4.24.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from nbformat->import-ipynb) (5.8.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->IPython->import-ipynb) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.25.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import-ipynb) (4.3.8)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->IPython->import-ipynb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->import-ipynb) (0.2.13)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat->import-ipynb) (4.14.0)\n",
            "Downloading import_ipynb-0.2-py3-none-any.whl (4.0 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, import-ipynb\n",
            "Successfully installed import-ipynb-0.2 jedi-0.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 1: Setup and Imports\n",
        "# ==============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import skew\n",
        "from scipy.stats import norm\n",
        "from scipy.special import boxcox1p\n",
        "import import_ipynb\n",
        "import mlpyutils\n",
        "\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor"
      ],
      "metadata": {
        "id": "bno_ifrxmVZF",
        "outputId": "9e839cd9-ac34-4503-fd61-3e8b65c52164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'import_ipynb'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2539107552>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mboxcox1p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mimport_ipynb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmlpyutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'import_ipynb'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For stacking\n",
        "from mlxtend.regressor import StackingCVRegressor\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set some options for pandas and plotting\n",
        "pd.set_option('display.max_columns', None)\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "print(\"Libraries imported successfully.\")"
      ],
      "metadata": {
        "id": "w46n0T7p7HYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 2: Data Loading and Initial Setup\n",
        "# ==============================================================================\n",
        "# Load the datasets\n",
        "train_df = pd.read_csv('./sample_data/train.csv')\n",
        "test_df = pd.read_csv('./sample_data/test.csv')\n",
        "\n",
        "print(f\"Train set shape: {train_df.shape}\")\n",
        "print(f\"Test set shape: {test_df.shape}\")\n",
        "\n",
        "# Save the 'Id' column before splitting and dropping\n",
        "# original_train_id = train_df['Id']\n",
        "\n",
        "# Randomly sample 100 rows for the test set and drop them from the training set\n",
        "# test_set_from_train = train_df.sample(n=100, random_state=42) # Use a random_state for reproducibility\n",
        "# train_df = train_df.drop(test_set_from_train.index)"
      ],
      "metadata": {
        "id": "aiU4yDPgKHSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the 'Id' column for submission and then drop it\n",
        "train_id = train_df['Id']\n",
        "test_id = test_df['Id']\n",
        "\n",
        "# Separate the 'Id' and 'SalePrice' from the new test set. Used to test the model from Train.csv\n",
        "#test_id = test_set_from_train['Id']\n",
        "# test_sale_price = test_set_from_train['SalePrice'] # Keep original SalePrice for evaluation\n",
        "\n",
        "# The 'Id' column is usually not useful for training machine learning models\n",
        "# as it's just an arbitrary identifier and doesn't contain predictive information.\n",
        "train_df.drop(\"Id\", axis=1, inplace=True)\n",
        "test_df.drop(\"Id\", axis=1, inplace=True)\n",
        "# test_df = test_set_from_train.drop(['Id', 'SalePrice'], axis=1) # Drop Id and SalePrice from test_df\n",
        "\n",
        "print(f\"Train set shape after splitting: {train_df.shape}\")\n",
        "print(f\"Generated Test set shape: {test_df.shape}\")\n",
        "#print(f\"Number of test samples generated from train: {len(test_set_from_train)}\")\n",
        "#print(\"Train IDs dropped. Test IDs and SalePrice for evaluation saved.\")"
      ],
      "metadata": {
        "id": "ETCoEZVK7Mia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_num = train_df.select_dtypes(include = ['float64', 'int64'])\n",
        "df_num.head()"
      ],
      "metadata": {
        "id": "3tuqGkOIrgrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_num.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8);"
      ],
      "metadata": {
        "id": "BhDxNNJSrh-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 3: Section 1 - Data Cleaning and Preparation\n",
        "# ==============================================================================\n",
        "# --- Visualizing the original target variable distribution ---\n",
        "plt.figure(figsize=(12, 5))\n",
        "sns.distplot(train_df['SalePrice'], fit=norm)\n",
        "plt.title('Original SalePrice Distribution (Right-Skewed)')\n",
        "plt.show()\n",
        "print(f\"Skewness of original SalePrice: {train_df['SalePrice'].skew():.2f}\")"
      ],
      "metadata": {
        "id": "4qOOwEcV_IKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the number of training examples for later splitting\n",
        "n_train = train_df.shape"
      ],
      "metadata": {
        "id": "22VDsIzaA_6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this code block before outlier removal\n",
        "# 3.1 Surgical Outlier Handling - Visualization Before Removal\n",
        "print(\"Visualizing GrLivArea vs SalePrice BEFORE outlier removal...\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(x = train_df['GrLivArea'], y = train_df['SalePrice'])\n",
        "plt.xlabel(\"GrLivArea\", fontsize=12)\n",
        "plt.ylabel(\"SalePrice\", fontsize=12)\n",
        "plt.title(\"GrLivArea vs SalePrice (Before Outlier Removal)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SHjHVQvW8v_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1 Surgical Outlier Handling - continued\n",
        "# As identified in the report, we remove two specific outliers in GrLivArea\n",
        "train_df = train_df.drop(train_df[(train_df['GrLivArea']>4000) & (train_df['GrLivArea']<300000)].index)\n",
        "print(f\"Outliers removed. New train set shape: {train_df.shape}\")\n",
        "\n",
        "# 3.2 Target Variable Transformation (Log Transform)\n",
        "# The competition uses RMSLE, so we log-transform the target variable 'SalePrice'\n",
        "# We use numpy.log1p which is log(1+x) for numerical stability\n",
        "y_train = train_df['SalePrice'] # Assign the Series to y_train\n",
        "y_train = np.log1p(y_train)     # Log-transform the Series\n",
        "\n",
        "# Visualize the transformation\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.distplot(y_train, fit=None, kde=True)\n",
        "plt.title('Log-Transformed SalePrice Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iiBd2V-P7XsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop SalePrice from the training set and concatenate with test set for unified preprocessing\n",
        "if 'SalePrice' in train_df.columns :\n",
        "    train_df.drop('SalePrice', axis=1, inplace=True)\n",
        "\n",
        "all_data = pd.concat((train_df, test_df)).reset_index(drop=True)\n",
        "print(f\"Combined dataset shape: {all_data.shape}\")\n",
        "\n",
        "# --- Strategic Imputation of Missing Data ---\n",
        "# This follows the detailed imputation strategy from the report.\n",
        "\n",
        "# Features where NaN means \"None\" (absence of the feature)\n",
        "for col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu',\n",
        "            'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
        "            'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
        "            'MasVnrType'):\n",
        "    all_data[col] = all_data[col].fillna('None')\n",
        "\n",
        "# Features where NaN means 0 (absence of the feature leads to zero measurement)\n",
        "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2',\n",
        "            'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n",
        "    all_data[col] = all_data[col].fillna(0)\n",
        "\n",
        "# Impute LotFrontage with the median of its neighborhood [4, 1, 5, 6, 7]\n",
        "all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(\n",
        "    lambda x: x.fillna(x.median()))\n",
        "\n",
        "# For other miscellaneous missing values, fill with the mode (most frequent value)\n",
        "for col in ('MSZoning', 'Electrical', 'Utilities', 'Exterior1st', 'Exterior2nd',\n",
        "            'SaleType', 'KitchenQual'):\n",
        "    all_data[col] = all_data[col].fillna(all_data[col].mode())\n",
        "\n",
        "# Functional: data description says NA means typical\n",
        "all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n",
        "\n",
        "# Check if any missing values remain\n",
        "print(f\"Remaining missing values: {all_data.isnull().sum().sum()}\")\n",
        "\n",
        "# Find rows with NaN values\n",
        "rows_with_nan = all_data[all_data.isnull().any(axis=1)]\n",
        "\n",
        "# Print the result\n",
        "print(rows_with_nan)"
      ],
      "metadata": {
        "id": "PfUSd4aY8XRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Section 2 - High-Impact Feature Engineering\n",
        "# ==============================================================================\n",
        "# This section creates new, more powerful features from the existing ones.\n",
        "\n",
        "# --- 2.1 Creating Holistic Features from Raw Components ---\n",
        "# Combine individual space-related features into a single, more powerful 'TotalSF'.\n",
        "# Assuming Total Bsmt SF, 1st Floor SF, and 2nd Floor SF are the intended features\n",
        "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
        "\n",
        "# Combine all bathroom features into a single 'TotalBaths' feature.\n",
        "# Assuming Full Bath (above ground), Half Bath (above ground), Bsmt Full Bath, and Bsmt Half Bath\n",
        "all_data['TotalBaths'] = all_data['FullBath'] + (0.5 * all_data['HalfBath']) + \\\n",
        "                           all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath'])\n",
        "\n",
        "# Combine all porch-related features into 'TotalPorchSF'.\n",
        "# Assuming Open Porch SF, Enclosed Porch, 3Ssn Porch, and Screen Porch\n",
        "all_data['TotalPorchSF'] = all_data['OpenPorchSF'] + all_data['EnclosedPorch'] + \\\n",
        "                             all_data['3SsnPorch'] + all_data['ScreenPorch']\n",
        "\n",
        "# --- 2.2 Capturing Temporal Dynamics ---\n",
        "# Create features that represent the age of the house and time since remodeling.\n",
        "# Assuming YearBuilt and YearRemodAdd\n",
        "all_data['AgeOfHouse'] = all_data['YrSold'] - all_data['YearBuilt']\n",
        "all_data['YearsSinceRemodel'] = all_data['YrSold'] - all_data['YearRemodAdd']\n",
        "# Correct any potential data errors where remodel is after sale\n",
        "all_data.loc[all_data['YearsSinceRemodel'] < 0, 'YearsSinceRemodel'] = 0\n",
        "\n",
        "# Create a feature indicating if the house was recently remodeled\n",
        "all_data['IsNewHouse'] = (all_data['YearBuilt'] == all_data['YrSold']).astype(int)\n",
        "\n",
        "# --- 2.3 Unlocking Non-Linearity with Polynomial and Interaction Features ---\n",
        "# Create squared terms for the most important features to help linear models capture non-linear relationships.\n",
        "all_data['OverallQual_sq'] = all_data['OverallQual']**2\n",
        "all_data['GrLivArea_sq'] = all_data['GrLivArea']**2\n",
        "# Create an interaction term between Overall Quality and Living Area.\n",
        "all_data['OverallQual_x_GrLivArea'] = all_data['OverallQual'] * all_data['GrLivArea']\n",
        "\n",
        "print(\"Feature engineering complete. New features added.\")"
      ],
      "metadata": {
        "id": "o8CUyqlch1eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 5: Section 3 - Skewness and Categorical Encoding\n",
        "# ==============================================================================\n",
        "# 5.1 Correcting Skewed Numerical Features\n",
        "numeric_feats = all_data.dtypes[all_data.dtypes!= \"object\"].index\n",
        "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
        "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
        "high_skew = skewness\n",
        "skewed_features = high_skew.index\n",
        "\n",
        "for feat in skewed_features:\n",
        "    all_data[feat] = boxcox1p(all_data[feat], 0.15) # Box-Cox transform\n",
        "\n",
        "print(f\"{len(skewed_features)} skewed numerical features transformed.\")\n",
        "\n",
        "# Check to see hwtehr there are NaN\n",
        "print(f\"NaN values in all_data: {all_data.isnull().any(axis=None)}\")\n",
        "\n",
        "# 5.2 Mastering Categorical Variables (One-Hot Encoding)\n",
        "# The report discusses advanced methods like target encoding. For a robust kick-starter,\n",
        "# one-hot encoding is a powerful and safe choice.\n",
        "final_data = pd.get_dummies(all_data).reset_index(drop=True)\n",
        "print(f\"Categorical features one-hot encoded. Final data shape: {final_data.shape}\")\n",
        "\n",
        "# Separate train and test sets again\n",
        "# Use the number of rows from the processed training data (before concatenation)\n",
        "# The original n_train stored the shape tuple, which caused the TypeError\n",
        "n_train_rows = len(train_df) # train_df here is the one *after* outlier removal and dropping 'SalePrice'\n",
        "X = final_data[:n_train_rows]\n",
        "X_test = final_data[n_train_rows:]\n",
        "\n",
        "print(f\"Final training features shape: {X.shape}\")\n",
        "print(f\"Final test features shape: {X_test.shape}\")\n",
        "\n",
        "# Check to see hwtehr there are NaN\n",
        "print(X.isnull().any(axis=None))\n",
        "print(X_test.isnull().any(axis=None))\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "print(\"\\nChecking for missing values per column in all_data:\")\n",
        "print(X_test.isnull().sum())"
      ],
      "metadata": {
        "id": "7Ga8ZedHDV2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Can you create the python function to print the scatter plot that accepts a plot title, test_sale_price, ensemble_pred based on the previous cell code\n",
        "\n",
        "def plot_scatter(plot_title, actual_values, predicted_values):\n",
        "    \"\"\"\n",
        "    Generates and displays a scatter plot comparing actual vs. predicted values.\n",
        "\n",
        "    Args:\n",
        "        plot_title (str): The title for the scatter plot.\n",
        "        actual_values (pd.Series or np.ndarray): The actual values.\n",
        "        predicted_values (pd.Series or np.ndarray): The predicted values.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(actual_values, predicted_values, alpha=0.5)\n",
        "    # Add a line representing perfect prediction\n",
        "    plt.plot([actual_values.min(), actual_values.max()],\n",
        "             [actual_values.min(), actual_values.max()], 'k--', lw=2)\n",
        "    plt.xlabel(\"Actual Values\")\n",
        "    plt.ylabel(\"Predicted Values\")\n",
        "    plt.title(plot_title)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def print_data(predicted_values):\n",
        "    output = pd.DataFrame({'Id': test_id,\n",
        "                       'SalePrice': predicted_values.squeeze()})\n",
        "    print(output.head())"
      ],
      "metadata": {
        "id": "18Q5UAzVDUya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course. Here is the explanation formatted as a text cell for a Google Colab notebook, using Markdown for structure and LaTeX for the formulas. You can copy and paste this directly into a Colab text cell.\n",
        "\n",
        "***\n",
        "\n",
        "# Understanding the Cost Function for Regularized Linear Regression\n",
        "\n",
        "Imagine you're training a new employee (our machine learning model) to become a real estate agent. This formula is the \"performance review\" we give the model to see how well it's doing.\n",
        "\n",
        "Our goal is to make the final score, **J**, as **low as possible**. A low score means our model is doing a great job at predicting house prices.\n",
        "\n",
        "The formula for the total score `J` is:\n",
        "$$J(\\vec{w}, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (f_{\\vec{w},b}(\\vec{x}^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2$$\n",
        "\n",
        "This performance score is made up of two distinct parts that work together.\n",
        "\n",
        "---\n",
        "\n",
        "### Part 1: The Accuracy Score\n",
        "$$\\text{Accuracy Score} = \\frac{1}{2m} \\sum_{i=1}^{m} (f_{\\vec{w},b}(\\vec{x}^{(i)}) - y^{(i)})^2$$\n",
        "\n",
        "**In plain English: \"How wrong were your predictions?\"**\n",
        "\n",
        "This first part just measures how far off the model's price predictions are from the actual prices.\n",
        "\n",
        "* `f(x)` is our model's **prediction** for a house's price.\n",
        "* `y` is the **actual** selling price of that house.\n",
        "* `(prediction - actual price)` is the **error**.\n",
        "* We **square the error `(...)²`** to make all errors positive and to heavily penalize huge mistakes.\n",
        "* The big **`∑`** sign means \"add up all these squared errors for every single house (`m` houses) in the dataset.\"\n",
        "\n",
        "Essentially, this part calculates the **average mistake** our model is making. To get a good score, we want this part to be as close to zero as possible.\n",
        "\n",
        "---\n",
        "\n",
        "### Part 2: The Simplicity Penalty\n",
        "$$\\text{Simplicity Penalty} = \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2$$\n",
        "\n",
        "**In plain English: \"Is your reasoning simple and robust?\"**\n",
        "\n",
        "This is the clever part that prevents our model from \"cheating.\" Imagine our model perfectly predicts every price in our dataset. If we ask how, it might give a ridiculously complex answer, like:\n",
        "\n",
        "*\"I used the square footage, the number of bedrooms, the phase of the moon, and the color of the neighbor's car.\"*\n",
        "\n",
        "This is a sign of **overfitting**. The model has memorized random details instead of learning the true patterns of what makes a house valuable. This second part of the formula adds a **penalty for being too complex**.\n",
        "\n",
        "* Each **`w`** represents the **importance** the model gives to a specific feature (e.g., \"square footage\").\n",
        "* `w²` and `∑` mean we're adding up the squared importance of **all the features**. This penalizes large, important-seeming features.\n",
        "* The Greek letter **`λ` (lambda)** is a **\"Strictness Knob\"**:\n",
        "    * If **`λ` is high**, the penalty is severe. The model is forced to keep its reasoning simple and only use small importance values (`w`), focusing on truly essential features.\n",
        "    * If **`λ` is zero**, this penalty disappears, and the model is free to become overly complex and overfit.\n",
        "\n",
        "---\n",
        "\n",
        "### Putting It All Together\n",
        "\n",
        "The entire formula is a balancing act:\n",
        "\n",
        "**`Total Score = (Average Prediction Mistake) + (Strictness Knob * Complexity Penalty)`**\n",
        "\n",
        "The model's job is to find the perfect settings for its parameters (`w` and `b`) that make this combined score as low as possible. It has to learn to be **accurate** (making the first part small) while also being **simple** and not \"cheating\" (making the second part small). This trade-off creates a much more robust and reliable model that will perform well on new houses it has never seen before.\n",
        "\n",
        "### How to support regulatization\n",
        "\n",
        "```python\n",
        "LightGBM works almost exactly the same way as XGBoost in this regard.\n",
        "The parameter names are even the same, which makes it easy to switch between them.\n",
        "\n",
        "reg_lambda: Controls L2 regularization.\n",
        "reg_alpha: Controls L1 regularization.\n",
        "\n",
        "# --- LightGBM Implementation ---\n",
        "# Again, you just set the parameters.\n",
        "lgbm_model = lgb.LGBMRegressor(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    reg_lambda=1.5,         # L2 Regularization\n",
        "    reg_alpha=0.5,          # L1 Regularization\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "The formula you showed is for L2 Regularization (penalizes the square of\n",
        "the weights). XGBoost also supports L1 Regularization (penalizes the absolute\n",
        "value of the weights).\n",
        "\n",
        "reg_lambda: This is the parameter for L2 regularization. It is the direct\n",
        "equivalent of the λ in your formula. It's sometimes just called lambda.\n",
        "reg_alpha: This is the parameter for L1 regularization.\n",
        "\n",
        "# --- XGBoost Implementation ---\n",
        "# You don't write the formula, you just set the parameters.\n",
        "# A higher reg_lambda means a stronger penalty on complexity.\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=500,        # Number of boosting rounds (trees)\n",
        "    learning_rate=0.05,      # How fast the model learns\n",
        "    reg_lambda=1.5,          # L2 Regularization (your formula's lambda)\n",
        "    reg_alpha=0.5,           # L1 Regularization (optional, often used too)\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "gbr_model = GradientBoostingRegressor(\n",
        "    n_estimators=200,      # Number of trees\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,           # Strong regularizer: limits tree complexity\n",
        "    min_samples_leaf=5,    # Strong regularizer: prevents fitting to noise\n",
        "    subsample=0.8,         # Strong regularizer: trains on a subset of data\n",
        "    ccp_alpha=0.01,        # Strong regularizer: prunes the trees\n",
        "    random_state=42\n",
        ")\n",
        "```\n"
      ],
      "metadata": {
        "id": "IDug_AIRACGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 6: Section 4 & 5 - Modeling (Stacking & Blending)\n",
        "# ==============================================================================\n",
        "# Define cross-validation strategy\n",
        "kfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define rmsle_cv for cross-validation scoring (Note: This function was defined but not explicitly used in the traceback path, keeping it for completeness)\n",
        "# This function is not causing the error, keeping it as is.\n",
        "def rmsle_cv(model):\n",
        "    return np.sqrt(-cross_val_score(model, X.values, y_train.values, scoring=\"neg_mean_squared_error\", cv=kfolds))\n",
        "\n",
        "# 4.1 Define Base Models\n",
        "# Using robust hyperparameters found in top public notebooks\n",
        "# These models will be the Level 0 learners in our stack\n",
        "\n",
        "# Linear Models (benefit from scaling)\n",
        "lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))\n",
        "elastic_net = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
        "ridge = make_pipeline(RobustScaler(), Ridge(alpha=10.0))\n",
        "\n",
        "# Gradient Boosting Models\n",
        "gbr = GradientBoostingRegressor(n_estimators=3000,     # Number of trees\n",
        "                                learning_rate=0.05,\n",
        "                                max_depth=4,           # Strong regularizer: limits tree complexity\n",
        "                                max_features='sqrt',\n",
        "                                min_samples_leaf=15,   # Strong regularizer: prevents fitting to noise\n",
        "                                min_samples_split=10,\n",
        "                                loss='huber',\n",
        "                                random_state =42)\n",
        "\n",
        "    subsample=0.8,         # Strong regularizer: trains on a subset of data\n",
        "    ccp_alpha=0.01,        # Strong regularizer: prunes the trees\n",
        "\n",
        "lgbm = LGBMRegressor(objective='regression',\n",
        "                       num_leaves=5,\n",
        "                       learning_rate=0.05, n_estimators=720,\n",
        "                       max_bin = 55, bagging_fraction = 0.8,\n",
        "                       bagging_freq = 5, feature_fraction = 0.2319,\n",
        "                       feature_fraction_seed=9, bagging_seed=9,\n",
        "                       min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
        "\n",
        "xgbr = XGBRegressor(learning_rate=0.05, n_estimators=2200,\n",
        "                     max_depth=3, min_child_weight=0,\n",
        "                     gamma=0, subsample=0.7,\n",
        "                     colsample_bytree=0.7,\n",
        "                     reg_alpha=0.005, nthread=-1,\n",
        "                     scale_pos_weight=1, seed=27)\n",
        "\n",
        "# 5.1 Stacking Implementation\n",
        "# The meta-model will be a regularized linear model (Lasso)\n",
        "# It learns to combine the predictions of the base models\n",
        "stacking_reg = StackingCVRegressor(regressors=(ridge, lasso, elastic_net, gbr, xgbr, lgbm),\n",
        "                                meta_regressor=lasso,\n",
        "                                use_features_in_secondary=True)\n",
        "\n",
        "print(\"Models defined. Ready for training.\")\n",
        "\n",
        "# 5.2 Training and Prediction\n",
        "print(\"Training Stacking Regressor...\")\n",
        "\n",
        "# The fit method call itself is correct, the issue is internal to mlxtend's\n",
        "# call to sklearn.model_selection.cross_val_predict.\n",
        "stacking_reg.fit(X.values, y_train.values)\n",
        "\n",
        "stack_pred_log = stacking_reg.predict(X_test.values)\n",
        "stack_pred = np.expm1(stack_pred_log)\n",
        "print(\"Stacking prediction complete.\")\n",
        "\n",
        "print(\"Training XGBoost for blending...\")\n",
        "xgbr.fit(X, y_train)\n",
        "xgb_pred_log = xgbr.predict(X_test)\n",
        "xgb_pred = np.expm1(xgb_pred_log)\n",
        "print(\"XGBoost prediction complete.\")\n",
        "\n",
        "print(\"Training LightGBM for blending...\")\n",
        "lgbm.fit(X, y_train)\n",
        "lgbm_pred_log = lgbm.predict(X_test)\n",
        "lgbm_pred = np.expm1(lgbm_pred_log)\n",
        "print(\"LightGBM prediction complete.\")\n",
        "\n",
        "# 5.3 Blending\n",
        "# As a final step, we blend the stacked model's predictions with the best\n",
        "# individual models' predictions. This often provides a small but crucial boost.\n",
        "ensemble_pred = (0.70 * stack_pred) + (0.15 * xgb_pred) + (0.15 * lgbm_pred)\n",
        "print(\"Ensemble prediction complete.\")"
      ],
      "metadata": {
        "id": "FGLsy09PD8CH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scatter(\"Actual vs. Ensemble Predicted SalePrice ( Uses all model )\", test_sale_price, ensemble_pred)"
      ],
      "metadata": {
        "id": "59ZXJqzKDaUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_data(ensemble_pred)\n",
        "\n",
        "\n",
        "\n",
        " \tId\tSalePrice\n",
        "0\t1461\t129616.96\n",
        "1\t1462\t171479.42\n",
        "2\t1463\t187823.16\n",
        "3\t1464\t192902.42\n",
        "4\t1465\t184622.08\n",
        "5\t1466\t174867.62\n",
        "6\t1467\t180376.66\n",
        "7\t1468\t171678.19\n",
        "8\t1469\t181069.75\n",
        "9\t1470\t122742.73"
      ],
      "metadata": {
        "id": "NK7M5TBYFEsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))\n",
        "# 5.1 Stacking Implementation\n",
        "# The meta-model will be a regularized linear model (Lasso)\n",
        "# It learns to combine the predictions of the base models\n",
        "stacking_reg = StackingCVRegressor(regressors=(ridge, lasso, elastic_net, gbr),\n",
        "                                meta_regressor=lasso,\n",
        "                                use_features_in_secondary=True)\n",
        "\n",
        "print(\"Models defined. Ready for training.\")\n",
        "\n",
        "# 5.2 Training and Prediction\n",
        "print(\"Training Stacking Regressor...\")\n",
        "\n",
        "# The fit method call itself is correct, the issue is internal to mlxtend's\n",
        "# call to sklearn.model_selection.cross_val_predict.\n",
        "stacking_reg.fit(X.values, y_train.values)\n",
        "\n",
        "stack_pred_log = stacking_reg.predict(X_test.values)\n",
        "stack_pred = np.expm1(stack_pred_log)\n",
        "print(\"Stacking prediction complete.\")\n",
        "\n",
        "\n",
        "# 5.3 Blending\n",
        "# As a final step, we blend the stacked model's predictions with the best\n",
        "# individual models' predictions. This often provides a small but crucial boost.\n",
        "ensemble_pred_rle_gbr = (stack_pred)\n",
        "print(\"Ensemble prediction complete.\")"
      ],
      "metadata": {
        "id": "kDM5h_cA5pku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scatter(\"Actual vs. Ensemble Predicted SalePrice ( ridge, lasso, elastic_net, gbr)\", test_sale_price, ensemble_pred_rle_gbr)"
      ],
      "metadata": {
        "id": "Xuttc0Z-Dmw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_data(ensemble_pred_rle_gbr)"
      ],
      "metadata": {
        "id": "oVZUOpBJGCpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))\n",
        "# 5.1 Stacking Implementation\n",
        "# The meta-model will be a regularized linear model (Lasso)\n",
        "# It learns to combine the predictions of the base models\n",
        "stacking_reg = StackingCVRegressor(regressors=(ridge, lasso, elastic_net),\n",
        "                                meta_regressor=lasso,\n",
        "                                use_features_in_secondary=True)\n",
        "\n",
        "print(\"Models defined. Ready for training.\")\n",
        "\n",
        "# 5.2 Training and Prediction\n",
        "print(\"Training Stacking Regressor...\")\n",
        "\n",
        "# The fit method call itself is correct, the issue is internal to mlxtend's\n",
        "# call to sklearn.model_selection.cross_val_predict.\n",
        "stacking_reg.fit(X.values, y_train.values)\n",
        "\n",
        "stack_pred_log = stacking_reg.predict(X_test.values)\n",
        "stack_pred = np.expm1(stack_pred_log)\n",
        "print(\"Stacking prediction complete.\")\n",
        "\n",
        "\n",
        "# 5.3 Blending\n",
        "# As a final step, we blend the stacked model's predictions with the best\n",
        "# individual models' predictions. This often provides a small but crucial boost.\n",
        "ensemble_pred_rle = (stack_pred)\n",
        "print(\"Ensemble prediction complete.\")"
      ],
      "metadata": {
        "id": "NrE461GwBgQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scatter(\"Actual vs. Ensemble Predicted SalePrice ( ridge, lasso, elastic_net, gbr)\", test_sale_price, ensemble_pred_rle)"
      ],
      "metadata": {
        "id": "ueEkO7kOGom7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_data(ensemble_pred_rle)"
      ],
      "metadata": {
        "id": "ZfIoq6PjGs4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 7: Submission\n",
        "# ==============================================================================\n",
        "# submission = pd.DataFrame()\n",
        "# submission['Id'] = test_id\n",
        "# submission = ensemble_pred\n",
        "\n",
        "# Generate submission file\n",
        "# submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "# print(\"Submission file created successfully: submission.csv\")\n",
        "# print(submission.head())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "jSzVjmzLmM77"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}